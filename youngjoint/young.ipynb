{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188c9aea-7441-438b-930b-493af291704f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据中...\n",
      "分析数据中...\n",
      "总评论数: 2389900\n",
      "总用户数: 209152\n",
      "总书籍数: 93267\n",
      "\n",
      "用户评论数分析:\n",
      "平均每个用户的评论数: 11.43\n",
      "最少评论数: 1, 最多评论数: 2438\n",
      "\n",
      "书籍评论数分析:\n",
      "平均每本书的评论数: 25.62\n",
      "最少评论数: 1, 最多评论数: 20756\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "# 读取 Goodreads 子类数据\n",
    "def load_reviews(file_path):\n",
    "    reviews = []\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            reviews.append(review)\n",
    "    return reviews\n",
    "\n",
    "# 统计基本信息\n",
    "def analyze_reviews(reviews):\n",
    "    user_count = defaultdict(int)\n",
    "    book_count = defaultdict(int)\n",
    "    \n",
    "    # 统计每个用户的评论次数和每本书的评论次数\n",
    "    for review in reviews:\n",
    "        user_id = review['user_id']\n",
    "        book_id = review['book_id']\n",
    "        user_count[user_id] += 1\n",
    "        book_count[book_id] += 1\n",
    "    \n",
    "    total_reviews = len(reviews)\n",
    "    total_users = len(user_count)\n",
    "    total_books = len(book_count)\n",
    "    \n",
    "    print(f\"总评论数: {total_reviews}\")\n",
    "    print(f\"总用户数: {total_users}\")\n",
    "    print(f\"总书籍数: {total_books}\")\n",
    "    \n",
    "    # 分析评论次数分布\n",
    "    user_review_counts = list(user_count.values())\n",
    "    book_review_counts = list(book_count.values())\n",
    "    \n",
    "    print(f\"\\n用户评论数分析:\")\n",
    "    print(f\"平均每个用户的评论数: {sum(user_review_counts) / len(user_review_counts):.2f}\")\n",
    "    print(f\"最少评论数: {min(user_review_counts)}, 最多评论数: {max(user_review_counts)}\")\n",
    "    \n",
    "    print(f\"\\n书籍评论数分析:\")\n",
    "    print(f\"平均每本书的评论数: {sum(book_review_counts) / len(book_review_counts):.2f}\")\n",
    "    print(f\"最少评论数: {min(book_review_counts)}, 最多评论数: {max(book_review_counts)}\")\n",
    "\n",
    "# 主流程\n",
    "file_path = '/workspace/goodreads/goodreads_reviews_young_adult.json.gz'\n",
    "\n",
    "print(\"加载数据中...\")\n",
    "reviews = load_reviews(file_path)\n",
    "\n",
    "print(\"分析数据中...\")\n",
    "analyze_reviews(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d88a964b-ea42-4288-a48a-bc5b20a92d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据中...\n",
      "最受欢迎的 1608 本书的评论次数分布:\n",
      "书籍ID: 11870085, 评论次数: 20756\n",
      "书籍ID: 2767052, 评论次数: 18617\n",
      "书籍ID: 7260188, 评论次数: 13536\n",
      "书籍ID: 6148028, 评论次数: 11904\n",
      "书籍ID: 13335037, 评论次数: 10743\n",
      "书籍ID: 41865, 评论次数: 10535\n",
      "书籍ID: 15745753, 评论次数: 9590\n",
      "书籍ID: 11235712, 评论次数: 9585\n",
      "书籍ID: 9460487, 评论次数: 9557\n",
      "书籍ID: 11735983, 评论次数: 9207\n",
      "\n",
      "用户在前1608本书中的阅读情况:\n",
      "用户ID: 8842281e1d1347389f2ab93d60773d4d, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: 7504b2aee1ecb5b2872d3da381c6c91e, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: f8a89075dc6de14857561522e729f82c, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: 704eb93a316aff687a93d5215882eb21, 阅读涉及的4000本书中的数量: 3\n",
      "用户ID: 012515e5802b2e0f42915118c90fa04b, 阅读涉及的4000本书中的数量: 31\n",
      "用户ID: f4d16ea4ac59af59d257631398af39f4, 阅读涉及的4000本书中的数量: 3\n",
      "用户ID: 01ec1a320ffded6b2dd47833f2c8e4fb, 阅读涉及的4000本书中的数量: 17\n",
      "用户ID: 4b3636a043e5c99fa27ac897ccfa1151, 阅读涉及的4000本书中的数量: 8\n",
      "用户ID: 903d4b859e86a1dd6d7640849cc7067c, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: afc070543f19028dc7e7f084a0079f72, 阅读涉及的4000本书中的数量: 2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "# 读取 Goodreads 子类数据\n",
    "def load_reviews(file_path):\n",
    "    reviews = []\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            reviews.append(review)\n",
    "    return reviews\n",
    "\n",
    "# 选出最受欢迎的前1608本书\n",
    "def select_top_books(reviews, top_n=1608):\n",
    "    book_count = defaultdict(int)\n",
    "    \n",
    "    # 统计每本书的评论次数\n",
    "    for review in reviews:\n",
    "        book_id = review['book_id']\n",
    "        book_count[book_id] += 1\n",
    "    \n",
    "    # 按评论次数排序，选出前 top_n 本书\n",
    "    top_books = sorted(book_count.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_book_ids = set([book_id for book_id, count in top_books])\n",
    "    \n",
    "    return top_books, top_book_ids\n",
    "\n",
    "# 统计每个用户在前4000本书中的阅读情况\n",
    "def analyze_user_reading(reviews, top_book_ids):\n",
    "    user_top_book_count = defaultdict(int)\n",
    "    \n",
    "    # 按用户统计其阅读的前4000本书的数量\n",
    "    for review in reviews:\n",
    "        user_id = review['user_id']\n",
    "        book_id = review['book_id']\n",
    "        if book_id in top_book_ids:\n",
    "            user_top_book_count[user_id] += 1\n",
    "    \n",
    "    return user_top_book_count\n",
    "\n",
    "# 主流程\n",
    "file_path = '/workspace/goodreads/goodreads_reviews_young_adult.json.gz'\n",
    "\n",
    "print(\"加载数据中...\")\n",
    "reviews = load_reviews(file_path)\n",
    "\n",
    "# 选出最受欢迎的前4000本书\n",
    "top_books, top_book_ids = select_top_books(reviews, top_n=2608)\n",
    "\n",
    "# 输出最受欢迎的书籍及其评论次数\n",
    "print(f\"最受欢迎的 1608 本书的评论次数分布:\")\n",
    "for book_id, count in top_books[:10]:  # 仅输出前10本书的评论数\n",
    "    print(f\"书籍ID: {book_id}, 评论次数: {count}\")\n",
    "\n",
    "# 统计每个用户在这 4000 本书中阅读的数量\n",
    "user_top_book_count = analyze_user_reading(reviews, top_book_ids)\n",
    "\n",
    "# 输出部分用户阅读情况\n",
    "print(f\"\\n用户在前1608本书中的阅读情况:\")\n",
    "for user_id, count in list(user_top_book_count.items())[:10]:  # 仅输出前10个用户\n",
    "    print(f\"用户ID: {user_id}, 阅读涉及的4000本书中的数量: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3a9fa99-fe1f-47f2-ab84-e1101c16046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读最多书的用户ID: aca760854b57ce2ec981df32e46dc96c, 阅读的4000本热门书中的数量: 681\n",
      "读最少书的用户ID: 78cd202c062444af70d62d7f5572a14e, 阅读的4000本热门书中的数量: 104\n",
      "\n",
      "前 1120 用户在1608本热门书中的阅读情况:\n",
      "用户ID: aca760854b57ce2ec981df32e46dc96c, 阅读涉及的4000本书中的数量: 681\n",
      "用户ID: 288dc8c9871098c8a1b680db829275b4, 阅读涉及的4000本书中的数量: 518\n",
      "用户ID: aed35dbc626957174ebedf3c555b63d0, 阅读涉及的4000本书中的数量: 516\n",
      "用户ID: 0d344a261de9ab42e62cf9b3b7c52cc4, 阅读涉及的4000本书中的数量: 470\n",
      "用户ID: d321b1bcf294bca33510816afd898eb3, 阅读涉及的4000本书中的数量: 458\n",
      "用户ID: 63eb5a9ea6fbce905e96dadf97e60c93, 阅读涉及的4000本书中的数量: 440\n",
      "用户ID: 884719ebf7dbd2977768e179358f6758, 阅读涉及的4000本书中的数量: 425\n",
      "用户ID: af9864c9e69963abb963fe2c90dd6f09, 阅读涉及的4000本书中的数量: 405\n",
      "用户ID: 19ff136f47089904d689e69e36c991d0, 阅读涉及的4000本书中的数量: 399\n",
      "用户ID: 667b94d4c7e0b014bb6ab3636999e712, 阅读涉及的4000本书中的数量: 398\n"
     ]
    }
   ],
   "source": [
    "# 按照用户阅读的热门书籍数量排序，并选出前 1000 个用户\n",
    "def get_top_n_users(user_top_book_count, top_n=1120):\n",
    "    sorted_users = sorted(user_top_book_count.items(), key=lambda x: x[1], reverse=True)  # 按照阅读量从大到小排序\n",
    "    top_users = sorted_users[:top_n]  # 选出前 top_n 个用户\n",
    "    return top_users\n",
    "\n",
    "# 显示读最多和读最少的用户的阅读量\n",
    "def display_min_max_user_reading(top_users):\n",
    "    max_read_user = top_users[0]\n",
    "    min_read_user = top_users[-1]\n",
    "    \n",
    "    print(f\"读最多书的用户ID: {max_read_user[0]}, 阅读的4000本热门书中的数量: {max_read_user[1]}\")\n",
    "    print(f\"读最少书的用户ID: {min_read_user[0]}, 阅读的4000本热门书中的数量: {min_read_user[1]}\")\n",
    "\n",
    "# 筛选出前 1000 个用户\n",
    "top_n = 1120\n",
    "top_users = get_top_n_users(user_top_book_count, top_n=top_n)\n",
    "\n",
    "# 显示读最多书和读最少书的用户的阅读量\n",
    "display_min_max_user_reading(top_users)\n",
    "\n",
    "# 输出前1000个用户的阅读情况\n",
    "print(f\"\\n前 {top_n} 用户在1608本热门书中的阅读情况:\")\n",
    "for user_id, count in top_users[:10]:  # 仅显示前10个用户的阅读情况\n",
    "    print(f\"用户ID: {user_id}, 阅读涉及的4000本书中的数量: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6a9537-6320-4adf-8d8d-872abe6dcae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying top 4000 books...\n",
      "Analyzing user reading patterns...\n",
      "Selecting top 1000 users...\n",
      "Processing and saving data...\n",
      "\n",
      "Top 4000 books:\n",
      "Most reviewed book: 20756 reviews\n",
      "Least reviewed book among top 4000: 225 reviews\n",
      "\n",
      "Top 1000 users:\n",
      "User with most books read: 681 books\n",
      "User with least books read among top 1000: 104 books\n",
      "\n",
      "Processed data saved to /workspace/young/processed_ba_user_sessions.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import heapq\n",
    "\n",
    "def parse_date(date_string):\n",
    "    if not date_string:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(date_string, \"%a %b %d %H:%M:%S %z %Y\").timestamp()\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def get_top_books(file_path, top_n=1608):\n",
    "    book_counts = Counter()\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            book_counts[review['book_id']] += 1\n",
    "    return dict(book_counts.most_common(top_n))\n",
    "\n",
    "def get_user_reading_counts(file_path, top_books):\n",
    "    user_reading_counts = defaultdict(int)\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            if review['book_id'] in top_books:\n",
    "                user_reading_counts[review['user_id']] += 1\n",
    "    return user_reading_counts\n",
    "\n",
    "def get_top_users(user_reading_counts, top_n=1120):\n",
    "    return dict(sorted(user_reading_counts.items(), key=lambda x: x[1], reverse=True)[:top_n])\n",
    "\n",
    "def load_book_titles(file_path):\n",
    "    book_titles = {}\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            book = json.loads(line)\n",
    "            book_titles[book['book_id']] = book['title']\n",
    "    return book_titles\n",
    "\n",
    "def process_and_save_data(reviews_file, books_file, output_file, top_books, top_users):\n",
    "    book_titles = load_book_titles(books_file)\n",
    "    user_data = defaultdict(list)\n",
    "    \n",
    "    with gzip.open(reviews_file, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            user_id = review['user_id']\n",
    "            book_id = review['book_id']\n",
    "            \n",
    "            if user_id in top_users and book_id in top_books:\n",
    "                read_at = parse_date(review.get('read_at')) or parse_date(review.get('date_added'))\n",
    "                title = book_titles.get(book_id, \"Unknown Title\")\n",
    "                user_data[user_id].append({\n",
    "                    'book_id': book_id,\n",
    "                    'title': title,\n",
    "                    'read_at': read_at\n",
    "                })\n",
    "    \n",
    "    # Sort each user's reading list by date\n",
    "    for user_id, books in user_data.items():\n",
    "        user_data[user_id] = sorted(books, key=lambda x: x['read_at'] or 0)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(user_data, f)\n",
    "\n",
    "# Main processing\n",
    "reviews_file = '/workspace/goodreads/goodreads_reviews_young_adult.json.gz'\n",
    "books_file = '/workspace/goodreads/goodreads_books.json.gz'\n",
    "output_file = '/workspace/young/processed_ba_user_sessions.json'\n",
    "\n",
    "print(\"Identifying top 4000 books...\")\n",
    "top_books = get_top_books(reviews_file)\n",
    "\n",
    "print(\"Analyzing user reading patterns...\")\n",
    "user_reading_counts = get_user_reading_counts(reviews_file, top_books)\n",
    "\n",
    "print(\"Selecting top 1000 users...\")\n",
    "top_users = get_top_users(user_reading_counts)\n",
    "\n",
    "print(\"Processing and saving data...\")\n",
    "process_and_save_data(reviews_file, books_file, output_file, top_books, top_users)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nTop 4000 books:\")\n",
    "print(f\"Most reviewed book: {max(top_books.values())} reviews\")\n",
    "print(f\"Least reviewed book among top 4000: {min(top_books.values())} reviews\")\n",
    "\n",
    "print(f\"\\nTop 1000 users:\")\n",
    "print(f\"User with most books read: {max(top_users.values())} books\")\n",
    "print(f\"User with least books read among top 1000: {min(top_users.values())} books\")\n",
    "\n",
    "print(f\"\\nProcessed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0f43f4e-0b3f-44c7-b97e-8e1f6bae8401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户数: 1120\n",
      "平均阅读量: 155.18\n",
      "中位数阅读量: 137\n",
      "最小阅读量: 104\n",
      "最大阅读量: 681\n",
      "\n",
      "平均阅读时间跨度: 2468.90 天\n",
      "\n",
      "最受欢迎的10本书:\n",
      "Cinder (The Lunar Chronicles, #1): 763 次阅读\n",
      "Divergent (Divergent, #1): 716 次阅读\n",
      "Throne of Glass (Throne of Glass, #1): 643 次阅读\n",
      "The Fault in Our Stars: 639 次阅读\n",
      "Scarlet (The Lunar Chronicles, #2): 627 次阅读\n",
      "The Raven Boys (The Raven Cycle, #1): 623 次阅读\n",
      "Anna and the French Kiss (Anna and the French Kiss, #1): 610 次阅读\n",
      "Insurgent (Divergent, #2): 603 次阅读\n",
      "Fangirl: 601 次阅读\n",
      "Shadow and Bone (The Grisha, #1): 594 次阅读\n",
      "\n",
      "随机用户 8284a77f03befda2ddb9f140ca12daa7 的前5条阅读记录:\n",
      "书名: Liar, 阅读时间: 2009-10-01 07:00:00\n",
      "书名: Monster, 阅读时间: 2010-01-01 08:00:00\n",
      "书名: Sold, 阅读时间: 2010-03-04 08:00:00\n",
      "书名: Crank (Crank, #1), 阅读时间: 2010-04-05 07:00:00\n",
      "书名: Glass (Crank, #2), 阅读时间: 2010-07-06 07:00:00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def analyze_data(data):\n",
    "    user_book_counts = {user: len(books) for user, books in data.items()}\n",
    "    reading_spans = {}\n",
    "    books_counter = Counter()\n",
    "\n",
    "    for user, books in data.items():\n",
    "        if books:\n",
    "            dates = [book['read_at'] for book in books if book['read_at']]\n",
    "            if dates:\n",
    "                reading_spans[user] = max(dates) - min(dates)\n",
    "        books_counter.update([book['title'] for book in books])\n",
    "\n",
    "    return user_book_counts, reading_spans, books_counter\n",
    "\n",
    "def print_statistics(user_book_counts, reading_spans, books_counter):\n",
    "    print(f\"用户数: {len(user_book_counts)}\")\n",
    "    print(f\"平均阅读量: {sum(user_book_counts.values()) / len(user_book_counts):.2f}\")\n",
    "    print(f\"中位数阅读量: {sorted(user_book_counts.values())[len(user_book_counts)//2]}\")\n",
    "    print(f\"最小阅读量: {min(user_book_counts.values())}\")\n",
    "    print(f\"最大阅读量: {max(user_book_counts.values())}\")\n",
    "\n",
    "    avg_span = sum(reading_spans.values()) / len(reading_spans) / (24 * 3600)  # 转换为天\n",
    "    print(f\"\\n平均阅读时间跨度: {avg_span:.2f} 天\")\n",
    "\n",
    "    print(\"\\n最受欢迎的10本书:\")\n",
    "    for book, count in books_counter.most_common(10):\n",
    "        print(f\"{book}: {count} 次阅读\")\n",
    "\n",
    "    # 随机选择一个用户进行样本检查\n",
    "    sample_user = random.choice(list(data.keys()))\n",
    "    print(f\"\\n随机用户 {sample_user} 的前5条阅读记录:\")\n",
    "    for book in data[sample_user][:5]:\n",
    "        print(f\"书名: {book['title']}, 阅读时间: {datetime.fromtimestamp(book['read_at'])}\")\n",
    "\n",
    "# 主处理流程\n",
    "file_path = '/workspace/young/processed_ba_user_sessions.json'\n",
    "data = load_data(file_path)\n",
    "user_book_counts, reading_spans, books_counter = analyze_data(data)\n",
    "print_statistics(user_book_counts, reading_spans, books_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b0d195f-6c4a-4ba9-a3fc-c2699de588aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "调试信息：前5个用户和其书籍映射结果\n",
      "新用户ID: 0, 阅读记录数: 215\n",
      "  新书籍ID: 0, 书名: Matched (Matched, #1), 阅读时间: 1339225200.0\n",
      "  新书籍ID: 1, 书名: Firelight (Firelight, #1), 阅读时间: 1339657200.0\n",
      "  新书籍ID: 2, 书名: Thirteen Reasons Why, 阅读时间: 1340262000.0\n",
      "  新书籍ID: 3, 书名: The Fault in Our Stars, 阅读时间: 1341903600.0\n",
      "  新书籍ID: 4, 书名: City of Bones (The Mortal Instruments, #1), 阅读时间: 1355817600.0\n",
      "新用户ID: 1, 阅读记录数: 139\n",
      "  新书籍ID: 155, 书名: Vampire Academy (Vampire Academy, #1), 阅读时间: 1252631367.0\n",
      "  新书籍ID: 215, 书名: After, 阅读时间: 1252738800.0\n",
      "  新书籍ID: 174, 书名: Frostbite (Vampire Academy, #2), 阅读时间: 1259654400.0\n",
      "  新书籍ID: 216, 书名: Liar, 阅读时间: 1260141372.0\n",
      "  新书籍ID: 217, 书名: I Heart You, You Haunt Me, 阅读时间: 1262332800.0\n",
      "新用户ID: 2, 阅读记录数: 108\n",
      "  新书籍ID: 324, 书名: The Looking Glass Wars (The Looking Glass Wars, #1), 阅读时间: 1191913200.0\n",
      "  新书籍ID: 325, 书名: The Subtle Knife (His Dark Materials, #2), 阅读时间: 1204963200.0\n",
      "  新书籍ID: 326, 书名: Graceling (Graceling Realm, #1), 阅读时间: 1221894000.0\n",
      "  新书籍ID: 327, 书名: Eon: Dragoneye Reborn (Eon, #1), 阅读时间: 1228809600.0\n",
      "  新书籍ID: 328, 书名: Bones of Faerie (Bones of Faerie, #1), 阅读时间: 1234166400.0\n",
      "新用户ID: 3, 阅读记录数: 107\n",
      "  新书籍ID: 413, 书名: Pandemonium (Delirium, #2), 阅读时间: 1341730800.0\n",
      "  新书籍ID: 414, 书名: I'll Give You the Sun, 阅读时间: 1423549284.0\n",
      "  新书籍ID: 415, 书名: Rebel Belle (Rebel Belle, #1), 阅读时间: 1423934577.0\n",
      "  新书籍ID: 158, 书名: Cinder (The Lunar Chronicles, #1), 阅读时间: 1424204470.0\n",
      "  新书籍ID: 180, 书名: Scarlet (The Lunar Chronicles, #2), 阅读时间: 1424358824.0\n",
      "新用户ID: 4, 阅读记录数: 192\n",
      "  新书籍ID: 10, 书名: City of Fallen Angels (The Mortal Instruments, #4), 阅读时间: 1337238000.0\n",
      "  新书籍ID: 26, 书名: City of Lost Souls (The Mortal Instruments, #5), 阅读时间: 1337497200.0\n",
      "  新书籍ID: 470, 书名: The Girl of Fire and Thorns (Fire and Thorns, #1), 阅读时间: 1342249200.0\n",
      "  新书籍ID: 2, 书名: Thirteen Reasons Why, 阅读时间: 1344153939.0\n",
      "  新书籍ID: 471, 书名: The Perks of Being a Wallflower, 阅读时间: 1348038000.0\n",
      "\n",
      "调试信息：前5个用户ID映射\n",
      "旧用户ID: 4a44f603cc3df339acc48590044a2db0, 新用户ID: 0\n",
      "旧用户ID: 5cca1dd30cd5a98c1c8e731839265ccf, 新用户ID: 1\n",
      "旧用户ID: 8bf745a1e2b3ec721ad079990111f114, 新用户ID: 2\n",
      "旧用户ID: b4226eb970603b17a0d5233b582b5a32, 新用户ID: 3\n",
      "旧用户ID: 1c845473e18c23f917126cb29bc8d243, 新用户ID: 4\n",
      "\n",
      "调试信息：前5个书籍标题映射\n",
      "书名: Matched (Matched, #1), 新书籍ID: 0\n",
      "书名: Firelight (Firelight, #1), 新书籍ID: 1\n",
      "书名: Thirteen Reasons Why, 新书籍ID: 2\n",
      "书名: The Fault in Our Stars, 新书籍ID: 3\n",
      "书名: City of Bones (The Mortal Instruments, #1), 新书籍ID: 4\n",
      "处理完成，用户和书籍ID已更新，id2name.txt 文件已生成。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def remap_ids(data):\n",
    "    user_id_map = {}\n",
    "    book_title_map = {}  # 使用书名作为键\n",
    "    new_user_id = 0\n",
    "    new_book_id = 0\n",
    "    \n",
    "    remapped_data = {}\n",
    "    for user, books in data.items():\n",
    "        if user not in user_id_map:\n",
    "            user_id_map[user] = new_user_id\n",
    "            new_user_id += 1\n",
    "        \n",
    "        new_books = []\n",
    "        for book in books:\n",
    "            title = book['title']\n",
    "            # 如果书名没有映射过，分配新的ID\n",
    "            if title not in book_title_map:\n",
    "                book_title_map[title] = new_book_id\n",
    "                new_book_id += 1\n",
    "            \n",
    "            # 替换书籍ID\n",
    "            new_books.append({\n",
    "                'book_id': book_title_map[title],\n",
    "                'title': title,\n",
    "                'read_at': book['read_at']\n",
    "            })\n",
    "        \n",
    "        remapped_data[user_id_map[user]] = new_books\n",
    "\n",
    "    # 调试信息\n",
    "    print(\"\\n调试信息：前5个用户和其书籍映射结果\")\n",
    "    for i, (user, books) in enumerate(remapped_data.items()):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        print(f\"新用户ID: {user}, 阅读记录数: {len(books)}\")\n",
    "        for book in books[:5]:\n",
    "            print(f\"  新书籍ID: {book['book_id']}, 书名: {book['title']}, 阅读时间: {book['read_at']}\")\n",
    "    \n",
    "    return remapped_data, user_id_map, book_title_map\n",
    "\n",
    "def generate_id2name(book_title_map, output_file):\n",
    "    sorted_books = sorted(book_title_map.items(), key=lambda x: x[1])\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for title, new_id in sorted_books:\n",
    "            f.write(f\"{new_id}:: {title}\\n\")\n",
    "\n",
    "# 主流程\n",
    "file_path = '/workspace/young/processed_ba_user_sessions.json'\n",
    "data = load_data(file_path)\n",
    "\n",
    "remapped_data, user_id_map, book_title_map = remap_ids(data)\n",
    "\n",
    "with open('/workspace/young/remapped_ya_user_sessions.json', 'w') as f:\n",
    "    json.dump(remapped_data, f, indent=2)\n",
    "\n",
    "generate_id2name(book_title_map, '/workspace/young/id2name.txt')\n",
    "\n",
    "with open('/workspace/young/user_id_map.json', 'w') as f:\n",
    "    json.dump(user_id_map, f, indent=2)\n",
    "\n",
    "# 调试信息\n",
    "print(\"\\n调试信息：前5个用户ID映射\")\n",
    "for i, (old_id, new_id) in enumerate(user_id_map.items()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"旧用户ID: {old_id}, 新用户ID: {new_id}\")\n",
    "\n",
    "print(\"\\n调试信息：前5个书籍标题映射\")\n",
    "for i, (title, new_id) in enumerate(book_title_map.items()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"书名: {title}, 新书籍ID: {new_id}\")\n",
    "\n",
    "print(\"处理完成，用户和书籍ID已更新，id2name.txt 文件已生成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc4d37e4-f364-488f-9a25-6f77e332ff85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "调试信息：前5个用户和其书籍映射结果\n",
      "新用户ID: 0, 阅读记录数: 69\n",
      "  新书籍ID: 0, 书名: Affinity, 阅读时间: 1341371308.0\n",
      "  新书籍ID: 1, 书名: A Spy in the House (The Agency, #1), 阅读时间: 1351580400.0\n",
      "  新书籍ID: 2, 书名: The Body at the Tower (The Agency, #2), 阅读时间: 1351839600.0\n",
      "  新书籍ID: 3, 书名: The Cater Street Hangman (Charlotte & Thomas Pitt, #1), 阅读时间: 1360170255.0\n",
      "  新书籍ID: 4, 书名: The Apothecary's Daughter, 阅读时间: 1360742400.0\n",
      "新用户ID: 1, 阅读记录数: 50\n",
      "  新书籍ID: 69, 书名: Atonement, 阅读时间: 1248850800.0\n",
      "  新书籍ID: 70, 书名: Roll of Thunder, Hear My Cry (Logans, #4), 阅读时间: 1251788400.0\n",
      "  新书籍ID: 71, 书名: Out of the Dust, 阅读时间: 1259740800.0\n",
      "  新书籍ID: 72, 书名: The Guernsey Literary and Potato Peel Pie Society, 阅读时间: 1273042800.0\n",
      "  新书籍ID: 73, 书名: Prayers for Sale, 阅读时间: 1273906800.0\n",
      "新用户ID: 2, 阅读记录数: 49\n",
      "  新书籍ID: 116, 书名: The Wedding (Lairds' Fiancées, #2), 阅读时间: 874306800.0\n",
      "  新书籍ID: 117, 书名: sTORI Telling, 阅读时间: 1239260400.0\n",
      "  新书籍ID: 118, 书名: A Reliable Wife, 阅读时间: 1246690800.0\n",
      "  新书籍ID: 119, 书名: To Tame a Highland Warrior (Highlander, #2), 阅读时间: 1256713200.0\n",
      "  新书籍ID: 72, 书名: The Guernsey Literary and Potato Peel Pie Society, 阅读时间: 1261641600.0\n",
      "新用户ID: 3, 阅读记录数: 55\n",
      "  新书籍ID: 95, 书名: The Secret Keeper, 阅读时间: 1355040000.0\n",
      "  新书籍ID: 160, 书名: Let's Pretend This Never Happened: A Mostly True Memoir, 阅读时间: 1356508800.0\n",
      "  新书籍ID: 131, 书名: The Winter Sea (Slains, #1), 阅读时间: 1369033200.0\n",
      "  新书籍ID: 76, 书名: To Kill a Mockingbird, 阅读时间: 1376228266.0\n",
      "  新书籍ID: 161, 书名: Bonhoeffer: Pastor, Martyr, Prophet, Spy, 阅读时间: 1377500400.0\n",
      "新用户ID: 4, 阅读记录数: 58\n",
      "  新书籍ID: 196, 书名: The Thorn Birds, 阅读时间: 662716800.0\n",
      "  新书籍ID: 197, 书名: Zorro, 阅读时间: 1193900400.0\n",
      "  新书籍ID: 32, 书名: The Secret History of the Pink Carnation (Pink Carnation, #1), 阅读时间: 1201852800.0\n",
      "  新书籍ID: 198, 书名: The English Patient, 阅读时间: 1206065565.0\n",
      "  新书籍ID: 199, 书名: The Shelters of Stone (Earth's Children, #5), 阅读时间: 1206255600.0\n",
      "\n",
      "调试信息：前5个用户ID映射\n",
      "旧用户ID: 792500e85277fa7ada535de23e7eb4c3, 新用户ID: 0\n",
      "旧用户ID: fc0a0792fd1c30427acdbfecbf5b0a20, 新用户ID: 1\n",
      "旧用户ID: ab2fadb5c7bbe55c80406d2b3692e969, 新用户ID: 2\n",
      "旧用户ID: 3b3f26019b3a5dbecb49c5faf1abce4c, 新用户ID: 3\n",
      "旧用户ID: d6804aa6e3a96b8e1104b8b9ac3fe882, 新用户ID: 4\n",
      "\n",
      "调试信息：前5个书籍ID映射\n",
      "旧书籍ID: 72929, 新书籍ID: 0, 书名: Affinity\n",
      "旧书籍ID: 6698199, 新书籍ID: 1, 书名: A Spy in the House (The Agency, #1)\n",
      "旧书籍ID: 7507889, 新书籍ID: 2, 书名: The Body at the Tower (The Agency, #2)\n",
      "旧书籍ID: 853180, 新书籍ID: 3, 书名: The Cater Street Hangman (Charlotte & Thomas Pitt, #1)\n",
      "旧书籍ID: 3870943, 新书籍ID: 4, 书名: The Apothecary's Daughter\n",
      "处理完成，用户和书籍ID已更新，id2name.txt 文件已生成。\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# # 读取数据\n",
    "# def load_data(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         return json.load(f)\n",
    "\n",
    "# # 重新映射用户ID和书籍ID\n",
    "# def remap_ids(data):\n",
    "#     user_id_map = {}\n",
    "#     book_id_map = {}\n",
    "#     new_user_id = 0\n",
    "#     new_book_id = 0\n",
    "    \n",
    "#     remapped_data = {}\n",
    "\n",
    "#     for user, books in data.items():\n",
    "#         # 如果用户ID没有映射过，分配新的ID\n",
    "#         if user not in user_id_map:\n",
    "#             user_id_map[user] = new_user_id\n",
    "#             new_user_id += 1\n",
    "        \n",
    "#         new_books = []\n",
    "#         for book in books:\n",
    "#             book_id = book['book_id']\n",
    "#             title = book['title']\n",
    "#             # 如果书籍ID没有映射过，分配新的ID\n",
    "#             if book_id not in book_id_map:\n",
    "#                 book_id_map[book_id] = (new_book_id, title)  # 保存book_id和title\n",
    "#                 new_book_id += 1\n",
    "            \n",
    "#             # 替换书籍ID\n",
    "#             new_books.append({\n",
    "#                 'book_id': book_id_map[book_id][0],  # 使用新ID\n",
    "#                 'title': title,\n",
    "#                 'read_at': book['read_at']\n",
    "#             })\n",
    "        \n",
    "#         # 使用新的用户ID和书籍ID\n",
    "#         remapped_data[user_id_map[user]] = new_books\n",
    "\n",
    "#     # 调试：打印前5个用户和书籍映射\n",
    "#     print(\"\\n调试信息：前5个用户和其书籍映射结果\")\n",
    "#     for i, (user, books) in enumerate(remapped_data.items()):\n",
    "#         if i >= 5:\n",
    "#             break\n",
    "#         print(f\"新用户ID: {user}, 阅读记录数: {len(books)}\")\n",
    "#         for book in books[:5]:  # 打印前5条阅读记录\n",
    "#             print(f\"  新书籍ID: {book['book_id']}, 书名: {book['title']}, 阅读时间: {book['read_at']}\")\n",
    "    \n",
    "#     return remapped_data, user_id_map, book_id_map\n",
    "\n",
    "# # 生成 id2name.txt 文件\n",
    "# def generate_id2name(book_id_map, output_file):\n",
    "#     # 按书籍ID排序\n",
    "#     sorted_books = sorted(book_id_map.items(), key=lambda x: x[1][0])  # 按照新的ID排序\n",
    "#     with open(output_file, 'w') as f:\n",
    "#         for book_id, (new_id, title) in sorted_books:\n",
    "#             f.write(f\"{new_id}:: {title}\\n\")\n",
    "\n",
    "# # 主流程\n",
    "# file_path = '/workspace/goodreads/processed_ba_user_sessions.json'\n",
    "# data = load_data(file_path)\n",
    "\n",
    "# # 重新映射用户ID和书籍ID\n",
    "# remapped_data, user_id_map, book_id_map = remap_ids(data)\n",
    "\n",
    "# # 将处理后的数据保存回 JSON 文件\n",
    "# with open('/workspace/goodreads/remapped_ya_user_sessions.json', 'w') as f:\n",
    "#     json.dump(remapped_data, f, indent=2)\n",
    "\n",
    "# # 生成 id2name.txt 文件\n",
    "# generate_id2name(book_id_map, '/workspace/goodreads/id2name.txt')\n",
    "\n",
    "# # 保存用户ID映射表\n",
    "# with open('/workspace/goodreads/user_id_map.json', 'w') as f:\n",
    "#     json.dump(user_id_map, f, indent=2)\n",
    "\n",
    "# # 调试：检查映射表\n",
    "# print(\"\\n调试信息：前5个用户ID映射\")\n",
    "# for i, (old_id, new_id) in enumerate(user_id_map.items()):\n",
    "#     if i >= 5:\n",
    "#         break\n",
    "#     print(f\"旧用户ID: {old_id}, 新用户ID: {new_id}\")\n",
    "\n",
    "# print(\"\\n调试信息：前5个书籍ID映射\")\n",
    "# for i, (old_id, (new_id, title)) in enumerate(book_id_map.items()):\n",
    "#     if i >= 5:\n",
    "#         break\n",
    "#     print(f\"旧书籍ID: {old_id}, 新书籍ID: {new_id}, 书名: {title}\")\n",
    "\n",
    "# print(\"处理完成，用户和书籍ID已更新，id2name.txt 文件已生成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "677d106f-9da9-47ae-99bb-cf04f75e3d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集生成完成，总记录数: 138420\n",
      "前5条训练集记录: \n",
      "                                                 seq  len_seq  next\n",
      "0  [298, 982, 307, 826, 127, 1179, 944, 855, 1097...       10   568\n",
      "1  [542, 797, 517, 715, 549, 1356, 1432, 491, 146...       10  1459\n",
      "2  [573, 1028, 832, 508, 939, 62, 1252, 419, 429,...       10   148\n",
      "3    [49, 51, 262, 1025, 471, 794, 658, 0, 1149, 33]       10   675\n",
      "4   [0, 594, 1025, 412, 604, 507, 4, 10, 1084, 1041]       10   949\n",
      "验证/测试集生成完成，总记录数: 112\n",
      "前5条验证/测试集记录: \n",
      "                                                 seq  len_seq  next\n",
      "0  [113, 760, 550, 801, 170, 838, 505, 1055, 1282...       10   199\n",
      "1  [187, 91, 1325, 422, 279, 194, 385, 613, 517, ...       10   129\n",
      "2  [189, 540, 352, 867, 202, 538, 203, 123, 1311,...       10   630\n",
      "3  [394, 327, 1350, 866, 93, 300, 408, 469, 448, ...       10   798\n",
      "4  [202, 407, 410, 554, 1351, 657, 199, 661, 559,...       10  1272\n",
      "验证/测试集生成完成，总记录数: 112\n",
      "前5条验证/测试集记录: \n",
      "                                                 seq  len_seq  next\n",
      "0  [207, 555, 1271, 204, 922, 923, 411, 772, 195,...       10   559\n",
      "1  [163, 921, 138, 801, 204, 469, 195, 868, 199, ...       10   733\n",
      "2  [408, 1188, 555, 661, 1418, 557, 1109, 766, 80...       10   963\n",
      "3  [106, 398, 749, 168, 496, 159, 429, 1109, 563,...       10   653\n",
      "4   [126, 429, 428, 767, 466, 285, 732, 9, 475, 759]       10   177\n",
      "训练集 集合长度: 138420\n",
      "训练集 集合中前 3 条记录:\n",
      "                                                 seq  len_seq  next\n",
      "0  [298, 982, 307, 826, 127, 1179, 944, 855, 1097...       10   568\n",
      "1  [542, 797, 517, 715, 549, 1356, 1432, 491, 146...       10  1459\n",
      "2  [573, 1028, 832, 508, 939, 62, 1252, 419, 429,...       10   148\n",
      "训练集 中 len_seq 的最小值: 1\n",
      "验证集 集合长度: 112\n",
      "验证集 集合中前 3 条记录:\n",
      "                                                 seq  len_seq  next\n",
      "0  [113, 760, 550, 801, 170, 838, 505, 1055, 1282...       10   199\n",
      "1  [187, 91, 1325, 422, 279, 194, 385, 613, 517, ...       10   129\n",
      "2  [189, 540, 352, 867, 202, 538, 203, 123, 1311,...       10   630\n",
      "验证集 中 len_seq 的最小值: 10\n",
      "测试集 集合长度: 112\n",
      "测试集 集合中前 3 条记录:\n",
      "                                                 seq  len_seq  next\n",
      "0  [207, 555, 1271, 204, 922, 923, 411, 772, 195,...       10   559\n",
      "1  [163, 921, 138, 801, 204, 469, 195, 868, 199, ...       10   733\n",
      "2  [408, 1188, 555, 661, 1418, 557, 1109, 766, 80...       10   963\n",
      "测试集 中 len_seq 的最小值: 10\n",
      "数据集生成并保存完成。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取数据\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# 填充历史序列的函数，将 pad_item 填充到序列末尾\n",
    "def pad_history(itemlist, length, pad_item):\n",
    "    if len(itemlist) >= length:\n",
    "        return itemlist[-length:]\n",
    "    else:\n",
    "        return itemlist + [pad_item] * (length - len(itemlist))\n",
    "\n",
    "# # 生成训练集的函数，并过滤掉 len_seq = 0 的记录\n",
    "# def generate_train_sequences(data, length=10, pad_item=3934):\n",
    "#     state, len_state, action = [], [], []\n",
    "    \n",
    "#     for user_id, books in data.items():\n",
    "#         history = []\n",
    "#         for index, book in enumerate(books):\n",
    "#             s = list(history)  # 复制当前的历史记录\n",
    "#             if len(history) > 0:  # 只生成有效的历史序列\n",
    "#                 len_state.append(len(s) if len(s) < length else length)  # 保存历史序列的长度\n",
    "#                 s = pad_history(s, length, pad_item)  # 填充或截取历史序列\n",
    "\n",
    "#                 state.append(s)\n",
    "#                 action.append(book['book_id'])  # 预测的下一本书\n",
    "\n",
    "#             # 更新历史记录\n",
    "#             history.append(book['book_id'])\n",
    "    \n",
    "#     # 创建 DataFrame 并确保索引从 0 开始\n",
    "#     train_df = pd.DataFrame({'seq': state, 'len_seq': len_state, 'next': action})\n",
    "#     train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     # 打印一些调试信息\n",
    "#     print(f\"训练集生成完成，总记录数: {len(train_df)}\")\n",
    "#     print(f\"前5条训练集记录: \\n{train_df.head()}\")\n",
    "\n",
    "#     return train_df\n",
    "\n",
    "# 生成训练集的函数，并过滤掉 len_seq = 0 的记录，且打乱顺序\n",
    "def generate_train_sequences(data, length=10, pad_item=1478):\n",
    "    state, len_state, action = [], [], []\n",
    "    \n",
    "    for user_id, books in data.items():\n",
    "        history = []\n",
    "        for index, book in enumerate(books):\n",
    "            s = list(history)  # 复制当前的历史记录\n",
    "            if len(history) > 0:  # 只生成有效的历史序列\n",
    "                len_state.append(len(s) if len(s) < length else length)  # 保存历史序列的长度\n",
    "                s = pad_history(s, length, pad_item)  # 填充或截取历史序列\n",
    "\n",
    "                state.append(s)\n",
    "                action.append(book['book_id'])  # 预测的下一本书\n",
    "\n",
    "            # 更新历史记录\n",
    "            history.append(book['book_id'])\n",
    "    \n",
    "    # 创建 DataFrame 并确保索引从 0 开始\n",
    "    train_df = pd.DataFrame({'seq': state, 'len_seq': len_state, 'next': action})\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 打乱数据\n",
    "    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # 打印一些调试信息\n",
    "    print(f\"训练集生成完成，总记录数: {len(train_df)}\")\n",
    "    print(f\"前5条训练集记录: \\n{train_df.head()}\")\n",
    "\n",
    "    return train_df\n",
    "\n",
    "# 生成验证集和测试集的函数，并过滤掉 len_seq = 0 的记录\n",
    "def generate_test_sequences(data, length=10, pad_item=1478):\n",
    "    state, len_state, action = [], [], []\n",
    "    \n",
    "    for user_id, books in data.items():\n",
    "        history = [book['book_id'] for book in books]\n",
    "        \n",
    "        if len(history) > 1:\n",
    "            s = history[:-1]  # 最后一条作为预测目标，之前的作为历史记录\n",
    "        else:\n",
    "            s = []\n",
    "\n",
    "        if len(s) > 0:  # 只生成有效的历史序列\n",
    "            len_state.append(len(s) if len(s) < length else length)  # 保存历史序列的长度\n",
    "            s = pad_history(s, length, pad_item)  # 填充或截取历史序列\n",
    "\n",
    "            state.append(s)\n",
    "            action.append(history[-1])  # 最后一条作为预测目标\n",
    "    \n",
    "    # 创建 DataFrame 并确保索引从 0 开始\n",
    "    test_df = pd.DataFrame({'seq': state, 'len_seq': len_state, 'next': action})\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 打印一些调试信息\n",
    "    print(f\"验证/测试集生成完成，总记录数: {len(test_df)}\")\n",
    "    print(f\"前5条验证/测试集记录: \\n{test_df.head()}\")\n",
    "\n",
    "    return test_df\n",
    "\n",
    "# 检查数据集\n",
    "def check_data(df, name):\n",
    "    print(f\"{name} 集合长度: {len(df)}\")\n",
    "    print(f\"{name} 集合中前 3 条记录:\\n{df.head(3)}\")\n",
    "    print(f\"{name} 中 len_seq 的最小值: {df['len_seq'].min()}\")\n",
    "    assert df['len_seq'].min() > 0, f\"{name} 集合中有 len_seq = 0 的记录！\"\n",
    "\n",
    "# 主流程\n",
    "file_path = '/workspace/young/remapped_ya_user_sessions.json'\n",
    "data = load_data(file_path)\n",
    "\n",
    "# 假设用户按 8:1:1 划分为训练、验证和测试集\n",
    "total_users = list(data.keys())\n",
    "fractions = [0.8, 0.1, 0.1]\n",
    "train_users, val_users, test_users = np.split(total_users, [int(0.8*len(total_users)), int(0.9*len(total_users))])\n",
    "\n",
    "# 根据用户划分生成训练集、验证集和测试集\n",
    "train_data = {user: data[user] for user in train_users}\n",
    "val_data = {user: data[user] for user in val_users}\n",
    "test_data = {user: data[user] for user in test_users}\n",
    "\n",
    "# 生成并保存训练集\n",
    "train_df = generate_train_sequences(train_data)\n",
    "train_df.to_pickle('/workspace/young/train_data.df')\n",
    "\n",
    "# 生成并保存验证集\n",
    "val_df = generate_test_sequences(val_data)\n",
    "val_df.to_pickle('/workspace/young/val_data.df')\n",
    "\n",
    "# 生成并保存测试集\n",
    "test_df = generate_test_sequences(test_data)\n",
    "test_df.to_pickle('/workspace/young/test_data.df')\n",
    "\n",
    "# 检查生成的 DataFrame\n",
    "check_data(train_df, \"训练集\")\n",
    "check_data(val_df, \"验证集\")\n",
    "check_data(test_df, \"测试集\")\n",
    "\n",
    "print(\"数据集生成并保存完成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9323f4bf-bc8f-4c23-8b6b-6d6c1c1daab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗完成，已保存到 /workspace/young/id2name_clean.txt\n",
      "清洗后条目数: 1478\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_id2name_remove_brackets(file_path, output_path):\n",
    "    cleaned_data = []\n",
    "    \n",
    "    # 定义一个正则表达式，匹配括号及其内部的内容\n",
    "    pattern = r'\\s*\\(.*?\\)'\n",
    "\n",
    "    # 读取并清洗数据\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('::')\n",
    "            if len(parts) == 2:  # 确保有 id 和 book_name 两部分\n",
    "                book_id, book_name = parts\n",
    "                if book_name:  # 确保书名不为空\n",
    "                    # 使用正则表达式去掉括号和其中的内容\n",
    "                    book_name_cleaned = re.sub(pattern, '', book_name).strip()\n",
    "                    cleaned_data.append(f\"{book_id}::{book_name_cleaned}\")\n",
    "    \n",
    "    # 保存清洗后的数据\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for line in cleaned_data:\n",
    "            f.write(f\"{line}\\n\")\n",
    "    \n",
    "    print(f\"清洗完成，已保存到 {output_path}\")\n",
    "    print(f\"清洗后条目数: {len(cleaned_data)}\")\n",
    "\n",
    "# 调用函数进行清洗\n",
    "file_path = '/workspace/young/id2name.txt'\n",
    "output_path = '/workspace/young/id2name_clean.txt'\n",
    "\n",
    "clean_id2name_remove_brackets(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae3034-e542-4a2b-978d-674335343254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
